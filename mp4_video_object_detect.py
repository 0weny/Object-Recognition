import cv2
import numpy as np
import mediapipe as mp
from ultralytics import YOLO
import os
import subprocess
import json
from collections import Counter
import sys

# ì…ë ¥ ì¸ì ë°›ê¸°
input_path = sys.argv[1]  # ì˜ˆ: uploads/input_20250726_203000.mp4
timestamp = sys.argv[2]   # ì˜ˆ: 20250726_203000

# ê²½ë¡œ ì„¤ì •
FINAL_VIDEO_PATH = f'static/outputs/output_final_{timestamp}.mp4'
JSON_PATH = f'static/outputs/object_summary_{timestamp}.json'
AUDIO_PATH = f'temp_audio_{timestamp}.aac'  # ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼
TEMP_VIDEO_PATH = f'temp_video_{timestamp}.mp4'  # ì„ì‹œ ì˜ìƒ íŒŒì¼

# YOLO ëª¨ë¸
model = YOLO('yolov8n.pt')

# MediaPipe ì–¼êµ´ íƒì§€
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(
    static_image_mode=False,
    max_num_faces=2,
    refine_landmarks=True,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles

# ê°ì²´ ì¹´ìš´íŠ¸
object_counter = Counter()

# ì˜ìƒ ì—´ê¸°
cap = cv2.VideoCapture(input_path)
if not cap.isOpened():
    print("âŒ ì˜ìƒ ì—´ê¸° ì‹¤íŒ¨")
    exit()

fps = cap.get(cv2.CAP_PROP_FPS)
w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(TEMP_VIDEO_PATH, fourcc, fps, (w, h))

print("ğŸ ì˜ìƒ ë¶„ì„ ì‹œì‘...")

frame_idx = 0
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    frame_idx += 1
    print(f"â–¶ í”„ë ˆì„ {frame_idx} ë¶„ì„ ì¤‘...")

    results = model.predict(frame, verbose=False)[0]
    for box in results.boxes:
        x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())
        cls = int(box.cls[0].cpu().numpy())
        conf = float(box.conf[0].cpu().numpy())
        label = model.model.names[cls]

        if conf > 0.5:
            object_counter[label] += 1
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(frame, f'{label} ({conf:.2f})', (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    face_results = face_mesh.process(rgb)

    if face_results.multi_face_landmarks:
        for face_landmarks in face_results.multi_face_landmarks:
            mp_drawing.draw_landmarks(
                image=frame,
                landmark_list=face_landmarks,
                connections=mp_face_mesh.FACEMESH_TESSELATION,
                landmark_drawing_spec=None,
                connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style()
            )
            mp_drawing.draw_landmarks(
                image=frame,
                landmark_list=face_landmarks,
                connections=mp_face_mesh.FACEMESH_CONTOURS,
                landmark_drawing_spec=None,
                connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style()
            )
            mp_drawing.draw_landmarks(
                image=frame,
                landmark_list=face_landmarks,
                connections=mp_face_mesh.FACEMESH_IRISES,
                landmark_drawing_spec=None,
                connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_iris_connections_style()
            )

    out.write(frame)

cap.release()
out.release()
cv2.destroyAllWindows()

# JSON ì €ì¥
with open(JSON_PATH, 'w', encoding='utf-8') as f:
    json.dump(object_counter.most_common(), f, indent=2, ensure_ascii=False)
print(f"ğŸ“Š ê°ì²´ í†µê³„ ì €ì¥: {JSON_PATH}")

# ì˜¤ë””ì˜¤ ì¶”ì¶œ
subprocess.run([
    "ffmpeg", "-y",
    "-i", input_path,
    "-vn", "-acodec", "copy", AUDIO_PATH
])

# ì˜ìƒ + ì˜¤ë””ì˜¤ ë³‘í•©
subprocess.run([
    "ffmpeg", "-y",
    "-i", TEMP_VIDEO_PATH,
    "-i", AUDIO_PATH,
    "-c:v", "copy", "-c:a", "aac", "-shortest",
    FINAL_VIDEO_PATH
])

# ì„ì‹œ íŒŒì¼ ì‚­ì œ
os.remove(TEMP_VIDEO_PATH)
os.remove(AUDIO_PATH)

print(f"âœ… ìµœì¢… ì˜ìƒ ì €ì¥ ì™„ë£Œ: {FINAL_VIDEO_PATH}")
